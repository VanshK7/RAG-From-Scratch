# -*- coding: utf-8 -*-
"""RAG_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xz157WKSYyY12W5ybQWvt5Y6SkoGZ2LZ

# RAG From Scratch

Creating and running a local RAG pipeline from scratch.

It will be private, fast and cost-effective.

## What I'll be building:

A RAG pipeline that enables us to chat with a large PDF document (1000+ pages long).

We'll write the code to:
1. Open a PDF document (you could use almost any PDF here).
2. Format the text of the PDF textbook ready for an embedding model (this process is known as text splitting/3.chunking).
3. Embed all of the chunks of text in the textbook and turn them into numerical representation which we can store for later.
4. Build a retrieval system that uses vector search to find relevant chunks of text based on a query.
5. Create a prompt that incorporates the retrieved pieces of text.
6. Generate an answer to a query based on passages from the textbook.

The above steps can broken down into two major sections:

1. Document preprocessing/embedding creation (steps 1-3).
2. Search and answer (steps 4-6).

## 1. Document/text processing and embedding creation

Ingredients:
* PDF document of choice (code can be adapted to other document formats)
* Embedding model of choice

Steps:
1. Import the PDF
2. Process the text for embedding (e.g. split into chunks of sentences).
3. Embed text chunks with embedding model.
4. Save embeddings locally

### Import PDF documents
"""

import os

pdf_path = "human-nutrition-text.pdf"

import fitz             # Using PyMuPDF for PDF formatting
from tqdm.auto import tqdm

def text_formatter(text: str) -> str:
    "Performs minor formatting on text"
    cleaned_text = text.replace("\n", " ").strip()
    return cleaned_text

def open_and_read_pdf(pdf_path: str) -> list[dict]:
    doc = fitz.open(pdf_path)
    pages_and_texts = []
    for page_number, page in tqdm(enumerate(doc)):
        text = page.get_text()
        text = text_formatter(text=text)
        pages_and_texts.append({'page_number': page_number - 41,                    # -41 because the first 41 pages are not relevant
                               "page_char_count": len(text),
                               "page_word_count": len(text.split(" ")),             # Splitting words on spaces
                               "page_sentence_count_raw": len(text.split(". ")),    # Splitting sentences on full stops followed by a space
                               "page_token_count": len(text) / 4,           # assuming 1 token is approx. 4 characters
                               "text": text})
    return pages_and_texts

pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)
pages_and_texts[:2]

# Checking a random sample
pages_and_texts[385]

import pandas as pd

df = pd.DataFrame(pages_and_texts)
df.head()

df.describe().round(2)

"""Why is token count important?

Token count is important as:
1. Embedding models and LLMs can't deal with infinite tokens.

For example, an embedding may have been trained to embed sequences of 384 tokens into numerical space (sentence-transformers `all-mpnet-base-v2`).

In case of LLMs, they cannot fit infinite elements in the context window.

### Further text processing (splitting pages into sentences)

2 ways to do it:
1. Splitting on ". " (what we've done)
2. Using a NLP library like spaCy
"""

from spacy.lang.en import English

nlp = English()

# Add a sentencizer pipeline
nlp.add_pipe("sentencizer")

# Create document instance as an example
doc = nlp("This is a sample sentence. This is another sample sentence")
assert(len(list(doc.sents)) == 2)           # Checking if the sentencizer is working

list(doc.sents)

pages_and_texts[250]

for item in tqdm(pages_and_texts):
    item["sentences"] = list(nlp(item["text"]).sents)

    # Ensuring all sentences are strings (default is spaCy datatype)
    item["sentences"] = [str(sentence) for sentence in item["sentences"]]

    # Count sentences
    item["page_sentence_count_spacy"] = len(item["sentences"])

df = pd.DataFrame(pages_and_texts)
df.describe().round(2)

"""### Chunking our sentences together

Chunking is the process of splitting larger pieces into smaller ones.

I'll split it into groups of 10 sentences.

What is the need for chunking?
1. So our texts are easier to filter (smaller groups of text are easier to inspect).
2. So our text chunks can fit into the embedding model context window.
3. So the contents passed to LLM can be more specific and focused.
"""

num_sentence_chunk_size = 10

# E.g if len = 25, split into 10 10 5
def split_list(input_list: list[str], split_size: int = num_sentence_chunk_size) -> list[list[str]]:
    return [input_list[i: i + split_size] for i in range(0, len(input_list), split_size)]

test_list = list(range(25))
split_list(test_list)

# Looping through pages and texts and split sentences into chunks
for item in tqdm(pages_and_texts):
    item["sentence_chunks"] = split_list(input_list=item["sentences"], split_size=num_sentence_chunk_size)
    item["num_chunks"] = len(item["sentence_chunks"])

df = pd.DataFrame(pages_and_texts)
df.describe().round(2)

"""### Splitting each chunk into its own item

By embedding each chunk of sentences into its numerical representation, we can dive deeper into the text sample that was used in our model,
"""

import re

# Splitting each chunk into its own item
pages_and_chunks = []
for item in tqdm(pages_and_texts):
    for sentence_chunk in item["sentence_chunks"]:
        chunk_dict = {}
        chunk_dict["page_number"] = item["page_number"]

        # Joining the sentence to form paragraphs
        joined_sentence_chunk = "".join(sentence_chunk).replace(" ", " ").strip()
        joined_sentence_chunk = re.sub(r'\.([A-Z])', r'. \1', joined_sentence_chunk)    # Adding space after full stop

        chunk_dict["sentence_chunk"] = joined_sentence_chunk
        chunk_dict["chunk_char_count"] = len(joined_sentence_chunk)
        chunk_dict["chunk_word_count"] = len([word for word in joined_sentence_chunk.split(" ")])
        chunk_dict["chunk_token_count"] = len(joined_sentence_chunk) / 4        # / 4 as 1 token = 4 characters

        pages_and_chunks.append(chunk_dict)

len(pages_and_chunks)

df = pd.DataFrame(pages_and_chunks)
df.describe().round(2)

"""### Filtering shorter chunks

As our embedding model can take upto 384 tokens and our max tokens in a chunk are 467.50, so some of the tokens in the chunk with the most number of tokens will be ignored.
"""

# Checking short chunks to see if they hold any important info
min_token_length = 30
for row in df[df["chunk_token_count"] <= min_token_length].sample(5).iterrows():
    print(f"Chunk token count: {row[1]['chunk_token_count']} | Text: {row[1]['sentence_chunk']}")

# Removing rows from our df with < 30 tokens
pages_and_chunks_over_min_token_len = df[df["chunk_token_count"] > min_token_length].to_dict(orient="records")      # orient = records stores each row as a dictionary

"""### Embedding our text chunks

We would like to turn our text chunks into embeddings (numbers).

Embeddings are *learned* representations.
"""

from sentence_transformers import SentenceTransformer
embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2")

import torch
device = "cuda" if torch.cuda.is_available() else "cpu"

# Creating a sample list of sentences
sentences = ["Sentence Transformers is a useful Python library",
             "Sentences can be embedded programatically",
             "I love pizza!"]

# Converting these sentences to embeddings
embeddings = embedding_model.encode(sentences)
embeddings_dict = dict(zip(sentences, embeddings))

# Check out the embeddings
for sentence, embedding in embeddings_dict.items():
    print(f"Sentence: {sentence}")
    print(f"Embedding: {embedding}\n")

embeddings.shape                # Each sentence is represented by 768 numbers"

embedding = embedding_model.encode("My favorite animal is the cow")
embedding

for item in tqdm(pages_and_chunks_over_min_token_len):
    item["embedding"] = embedding_model.encode(item["sentence_chunk"])

text_chunks = [item["sentence_chunk"] for item in pages_and_chunks_over_min_token_len]
text_chunks[424]

len(text_chunks)

# Embedding text chunks in batches
text_chunks_embeddings = embedding_model.encode(text_chunks, batch_size=32, convert_to_tensor=True, show_progress_bar=True)

"""### Save embeddings to file"""

text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)
embeddings_df_save_path = "text_chunks_and_embeddings_df.csv"
text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)

# Import saved file and view
text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)
text_chunks_and_embedding_df_load.head()

"""## 2. RAG - Search and Answer

RAG Goal: Retrieve relevant passages based on a query and use those passages to augment an input to an LLM so it can generate an output based on those relevant passages.

### Similarity Search

Embeddings can be used for almost any type of data (e.g. text, audio, images, etc.).

Comparing embeddings is known as similarity search/vector search/semantic search.

Our aim is to get relevant passages from the text for a given query.
"""

import random
import torch
import numpy as np
import pandas as pd

device = "cuda" if torch.cuda.is_available() else "cpu"

# Import texts and embeddings df
text_chunks_and_embeddings_df = pd.read_csv(embeddings_df_save_path)

# Convert embedding column to numpy array (it was converted to string when saved as csv)
text_chunks_and_embeddings_df["embedding"] = text_chunks_and_embeddings_df["embedding"].apply(lambda x: np.fromstring(x.strip("[]"), sep=" "))

# Convert our embeddings into a torch.tensor
embeddings = torch.tensor(np.stack(text_chunks_and_embeddings_df["embedding"].tolist(), axis=0), dtype=torch.float32).to(device)

# Convert texts and embedding df to list of dicts
pages_and_chunks = text_chunks_and_embeddings_df.to_dict(orient="records")

text_chunks_and_embeddings_df.head()

embeddings.shape

# Create model
from sentence_transformers import util, SentenceTransformer

embedding_model = SentenceTransformer(model_name_or_path="all-mpnet-base-v2", device=device)

"""Let's create a small semantic search pipeline.

We can do so with the following steps:
1. Define a query string
2. Turn the query string into an embedding
3. Perform a dot product or cosine similarity function between the text embeddings and the query embedding.
4. Sort the results in a descending order
"""

# 1. Define the query
query = "macronutrients functions"
print(f"Query: {query}")

# 2. Embed the query
query_embedding = embedding_model.encode(query, convert_to_tensor=True).to(device)

# 3. Get similarity scores with dot product (use cosine similarity if outputs of model aren't normalized)
dot_scores = util.dot_score(query_embedding, embeddings)[0]

# 4. Get the top k results (top 5)
top_results_dot_product = torch.topk(dot_scores, k=5)
top_results_dot_product

# Let's check it out
pages_and_chunks[42]["sentence_chunk"]          # It's related to macronutrients

"""As we can see, searching over embeddings is very fast.

But if we have hundreds of millions of records, it is beneficial to create an index.

An index is like letters in a dictionary. E.g. to search for rice, we'll start with r, then find words close to "ri..". An index helps narrow searches down.

A popular indexing library is FAISS which uses approximate nearest neighbour (ANN).
"""

# Making our vector search results pretty
import textwrap

def print_wrapped(text, wrap_length=100):
    wrapped_text = textwrap.fill(text, wrap_length)             # Wrap text to 100 characters
    print(wrapped_text)

query = "good foods for protein"
print(f"Query: {query}\n")
print("Results:")

# Loop through top results
for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):
    print(f"Score: {score:.4f}")
    print_wrapped(f"Text:\n{pages_and_chunks[idx]['sentence_chunk']}")
    print(f"Page number: {pages_and_chunks[idx]['page_number']}\n")

"""We can potentially improve the order of these results with a reranking model. A model that has specifically been trained to take search results (e.g. top 25 semantic results) and rank them in order from most likely top-1 to least likely."""

import fitz

# open PDF and load target
pdf_path = "human-nutrition-text.pdf"
doc = fitz.open(pdf_path)
page = doc.load_page(411 + 41)      # Adding 41 as the first 41 pages are not relevant

# Get the image of the page
img = page.get_pixmap(dpi=300)      # Higher the dpi, better image quality

doc.close()

# Convert pixmap to numpy array
img_array = np.frombuffer(img.samples_mv, dtype=np.uint8).reshape((img.h, img.w, img.n))

# Display the image using Matplotlib
import matplotlib.pyplot as plt
plt.figure(figsize=(13, 10))
plt.imshow(img_array)
plt.title(f"Query: {query}")
plt.axis("off")

"""### Similarity measures: dot product and cosine similarity

Closer vectors will have higher scores and vectors further away will have lower scores.

Vectors have direction (which way) and magnitude (how long is it?).
"""

import torch

def dot_product(vector1, vector2):
    return torch.dot(vector1, vector2)

def cosine_similarity(vector1, vector2):
    dot_product = torch.dot(vector1, vector2)

    # Get Euclidean/L2 norm
    norm_vector1 = torch.sqrt(torch.sum(vector1**2))
    norm_vector2 = torch.sqrt(torch.sum(vector2**2))

    return dot_product / (norm_vector1 * norm_vector2)

# Example vectors/tensors
vector1 = torch.tensor([1, 2, 3], dtype=torch.float32)
vector2 = torch.tensor([1, 2, 3], dtype=torch.float32)
vector3 = torch.tensor([4, 5, 6], dtype=torch.float32)
vector4 = torch.tensor([-1, -2, -3], dtype=torch.float32)

# Calculate dot product
print(f"Dot product between vector1 and vector2: {dot_product(vector1, vector2)}")      # Large positive value as they're exactly the same
print(f"Dot product between vector1 and vector3: {dot_product(vector1, vector3)}")
print(f"Dot product between vector1 and vector4: {dot_product(vector1, vector4)}")      # Large negative value as they're exactly opposite

# Cosine similarity
print(f"Cosine similarity between vector1 and vector2: {cosine_similarity(vector1, vector2)}")      # 1 as they're exactly the same
print(f"Cosine similarity between vector1 and vector3: {cosine_similarity(vector1, vector3)}")      # High but less than 1 as they're similar but not the same
print(f"Cosine similarity between vector1 and vector4: {cosine_similarity(vector1, vector4)}")      # -1 as they're exactly opposite

"""### Functionalizing our semantic search pipeline"""

def retrieve_relevant_resources(query: str,
                                embeddings: torch.Tensor,
                                model: SentenceTransformer = embedding_model,
                                n_resources_to_retun: int = 5,
                                print_time: bool = True):
    """
    Embeds a query with model and returns top k scores and indices from embeddings
    """

    # Embed the query
    query_embedding = model.encode(query, convert_to_tensor=True)

    # Get dot product scores on embeddings
    dot_scores = util.dot_score(query_embedding, embeddings)[0]

    scores, indices = torch.topk(input=dot_scores, k=n_resources_to_retun)
    return scores, indices

def print_top_results_and_scores(query: str,
                                 embeddings: torch.Tensor,
                                 pages_and_chunks: list[dict] = pages_and_chunks,
                                 n_resources_to_return: int = 5):
    """
    Finds relevant passages given a query and print them out along with their scores
    """
    scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings, n_resources_to_retun=n_resources_to_return)

    # Loop through zipped together scores and indices from torch.topk
    for score, idx in zip(scores, indices):
        print(f"Score: {score:.4f}")
        print("Text:")
        print_wrapped(pages_and_chunks[idx]["sentence_chunk"])
        print(f"Page number: {pages_and_chunks[idx]['page_number']}\n")

query = "high fiber food"
retrieve_relevant_resources(query=query, embeddings=embeddings)
print_top_results_and_scores(query=query, embeddings=embeddings)

"""### Getting an LLM for local generation

We can use a LLM API, but as we're focused on local RAG, we will use a local LLM.
"""

use_quantization_config = True              # Whether to quantize models
model_id = "google/gemma-2b-it"             # Which model to use

"""### Loading an LLM locally

To get a model running locally we need a few things:
1. A quantization config (optional) - A config on what precision to load the model in (e.g. 4 bit, 8 bit, etc.)
2. A model ID - This will tell transformers which model/tokenizer to load
3. A tokenizer - Turns texts into numbers ready for an LLM (not an embedding model)
4. An LLM model - this will be what we use to generate text based on an input!
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

use_quantization_config = True
HUGGINGFACE_HUB_TOKEN = "YOUR_TOKEN"
model_id = model_id

# Check CUDA availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Check CUDA availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

try:
    if torch.cuda.is_available() and use_quantization_config:
        # Define quantization configuration
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16
        )

        # Instantiate tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path=model_id,
            token=HUGGINGFACE_HUB_TOKEN
        )

        # Load model with quantization
        llm_model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path=model_id,
            torch_dtype=torch.float16,
            quantization_config=quantization_config,
            low_cpu_mem_usage=False,
            token=HUGGINGFACE_HUB_TOKEN
        )

    else:
        # Instantiate tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            pretrained_model_name_or_path=model_id,
            token=HUGGINGFACE_HUB_TOKEN
        )

        # Load model without quantization
        llm_model = AutoModelForCausalLM.from_pretrained(
            pretrained_model_name_or_path=model_id,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=False,
            token=HUGGINGFACE_HUB_TOKEN
        )

    # Move model to the correct device
    llm_model.to(device)
    print("Model loaded and moved to device successfully.")

except Exception as e:
    print(f"An error occurred: {e}")

def get_model_mem_size(model: torch.nn.Module):
    # Get model param and buffer sizes
    mem_params = sum([param.numel() * param.element_size() for param in model.parameters()])
    mem_buffers = sum([buf.numel() * buf.element_size() for buf in model.buffers()])

    # Calculate model sizes
    model_mem_bytes = mem_params + mem_buffers
    model_mem_mb = model_mem_bytes / (1024**2)
    model_mem_gb = model_mem_bytes / (1024**3)

    return {"model_mem_bytes": model_mem_bytes,
            "model_mem_mb": round(model_mem_mb, 2),
            "model_mem_gb": round(model_mem_gb, 2)}

get_model_mem_size(llm_model)

"""### Generating Text with LLM"""

input_text = "What are micronutrients, and what roles do they play in the human body?"
print(f"Input text:\n{input_text}")

# Create prompt template for instruction tuned model
dialogue_template = [
    {"role": "user",
     "content": input_text}
]

# Manually format the prompt
prompt = ""
for message in dialogue_template:
    if message['role'] == 'user':
        prompt += f"User: {message['content']}\n"

# Optionally add a generation prompt if needed
prompt += "Assistant:"

print(f"\nFormatted prompt:\n{[[prompt]]}")

# Tokenize the input text
input_ids = tokenizer(prompt,
                      return_tensors="pt").to(device)

# Generate outputs from local LM
outputs = llm_model.generate(**input_ids,           # ** means passing in the keys
                             max_new_tokens=256)
print(f"Model output tokens:\n{outputs[0]}")

# Decode the output tokens to text
outputs_decoded = tokenizer.decode(outputs[0])
print(f"Decoded model output:\n{outputs_decoded}")

# Sample nutrition questions generated by GPT4
gpt4_q = [
    "What are the macronutrients, and what roles do they play in the human body?",
    "How do vitamins and minerals differ in their roles and importance for health?",
    "What roles does fibre play in digestion? Name 5 fibre containing foods.",
    "Explain the concept of energy balance and its importance in weight management."
]

# Manually created q list
manual_questions = [
    "How often should infants be breastfed?",
    "What are symptoms of pellagra?",
    "How does saliva help with digestion?",
    "What is the RDI for protein per day?",
    "water soluble vitamins"
]

query_list = gpt4_q + manual_questions

import random

query = random.choice(query_list)
print(f"Query: {query}")

# Get just the scores and indices of top related results
scores, indices = retrieve_relevant_resources(query=query, embeddings=embeddings)

"""### Augmenting our prompt with context items

Now that we're done with retrieval and generation, let's focus on the augmentation part of RAG.

The concept of augmenting a prompt with context items is also referred as prompt engineering.

We're going to use a couple of techniques:
1. Give clear instructions.
2. Give a few examples of input/output (few shot prompting)
3. Give room to think (e.g. create a scratchpad/show your working/let's go step by step)
"""

def prompt_formatter(query: str,
                     context_items: list[dict]) -> str:

                     context = " - " + "\n- ".join([item["sentence_chunk"] for item in context_items])
                     base_prompt = """Based on the following context items, please answer the query.
                     Give yourself room to think by extracting relevant passages from the context before answering the query.
                     Don't return the thinking, only return the answer.
                     Make sure your answers are as explanatory as possible.
                     Use the following examples as reference (and only for reference) for the ideal answer style.
                     \nExample 1:
                     Query: What are the fat-soluble vitamins?
                     Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.
                     \nExample 2:
                     Query: What are the causes of type 2 diabetes?
                     Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.
                     \nExample 3:
                     Query: What is the importance of hydration for physical performance?
                     Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.
                     \nNow use the following context items to answer the user query:
                     {context}
                     \nRelevant passages: <extract relevant passages from the context here>
                     User query: {query}
                     Answer:"""
                     prompt = base_prompt.format(context=context, query=query)

                     # Create prompt template for instruction tuned model
                     dialogue_template = [
                         {'role': 'user',
                          'content': base_prompt,}
                     ]

                     # Apply the chat template
                     prompt = tokenizer.apply_chat_template(conversation=dialogue_template, tokenize=False, add_generation_prompt=False)
                     return prompt

query = random.choice(query_list)
print(f"Query:{query}")

# Get relevant resources
sources, indices = retrieve_relevant_resources(query=query, embeddings=embeddings)

# Create a list of context items
context_items = [pages_and_chunks[i] for i in indices]

# Format our prompt
prompt = prompt_formatter(query=query, context_items=context_items)
prompt

input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")

# Generate an output of tokens
outputs = llm_model.generate(**input_ids,
                             temperature=0.7,         # Higher the value, the more creative is the text
                             do_sample=True,          # Whether or not to use sampling
                             max_new_tokens=256)

# Turn the output tokens into text
output_text = tokenizer.decode(outputs[0])
print(f"Query: {query}")
print(f"RAG Answer:\n{output_text.replace(prompt, '')}")        # Getting the RAG answer and removing the prompt from it

"""### Functionize our LLM answering feature

Input -> Query
Output -> Generated Answer + Source Documents (optional)
"""

def ask(query,
        temperature=0.7,
        max_new_tokens=1024,
        format_answer_text=True,
        return_answer_only=True):
    """
    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.
    """

    # Get just the scores and indices of top related results
    scores, indices = retrieve_relevant_resources(query=query,
                                                  embeddings=embeddings)

    # Create a list of context items
    context_items = [pages_and_chunks[i] for i in indices]

    # Add score to context item
    for i, item in enumerate(context_items):
        item["score"] = scores[i].cpu()   # return score back to CPU

    # Format the prompt with context items
    prompt = prompt_formatter(query=query,
                              context_items=context_items)

    # Tokenize the prompt
    input_ids = tokenizer(prompt, return_tensors="pt").to("cuda")

    # Generate an output of tokens
    outputs = llm_model.generate(**input_ids,
                                 temperature=temperature,
                                 do_sample=True,
                                 max_new_tokens=max_new_tokens)

    # Turn the output tokens into text
    output_text = tokenizer.decode(outputs[0])

    if format_answer_text:
        # Replace special tokens and unnecessary help message
        output_text = output_text.replace(query, "").replace("<bos>", "").replace("<eos>", "").replace("<start_of_turn>", "").replace("user\nBased on the following context items, please answer the query.\n                     Give yourself room to think by extracting relevant passages from the context before answering the query.\n                     Don't return the thinking, only return the answer.\n                     Make sure your answers are as explanatory as possible.\n                     Use the following examples as reference for the ideal answer style.\n                     \nExample 1:\n                     Query: What are the fat-soluble vitamins?\n                     Answer: ", "").replace("user\nBased on the following context items, please answer the query.\n                     Give yourself room to think by extracting relevant passages from the context before answering the query.\n                     Don't return the thinking, only return the answer.\n                     Make sure your answers are as explanatory as possible.\n                     Use the following examples as reference (and only for reference) for the ideal answer style.\n                     \nExample 1:\n                     ", "")

    # Only return the answer without the context items
    if return_answer_only:
        return output_text

    return output_text, context_items

query = random.choice(query_list)
ask(query=query)

query = random.choice(query_list)
print(f"Query: {query}")

# Answer query with context and return context
answer, context_items = ask(query=query,
                            temperature=0.7,
                            max_new_tokens=512,
                            return_answer_only=False)

print(f"Answer:\n")
print_wrapped(answer)
print(f"Context items:")
context_items